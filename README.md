# RAGdoll Enterprise
## Multi-Service RAG (Retrieval-Augmented Generation) System

A containerized enterprise-grade RAG system featuring namespaced document retrieval, GPT-4o integration, and a modern React frontend.

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- OpenAI API key (optional, for GPT-4o integration)

### Setup
1. **Clone and configure**:
   ```bash
   git clone <repository-url>
   cd RAGdoll
   echo "OPENAI_API_KEY=your-api-key-here" > .env
   ```

2. **Build and run**:
   ```bash
   docker-compose -f docker-compose.fast.yml up --build
   ```

3. **Access the application**:
   - Frontend (TParty): http://localhost:3000
   - API Documentation: http://localhost:8000/docs
   - Default login: `admin` / `admin123`

## ğŸ—ï¸ Architecture

### Services
- **RAGdoll API** (Port 8000): FastAPI backend with JWT authentication
- **TParty Frontend** (Port 3000): React.js chat interface
- **Redis** (Port 6379): Caching layer

### Features
- âœ… **Namespaced Document Storage**: Organize documents by department/domain
- âœ… **BGE-M3 Embeddings**: High-quality semantic search (offline-capable)
- âœ… **FAISS Vector Search**: Sub-second document retrieval
- âœ… **JWT Authentication**: Secure API access
- âœ… **Redis Caching**: Performance optimization
- âœ… **Docker Containerization**: Easy deployment
- ğŸ”§ **GPT-4o Integration**: (SSL configuration required in corporate environments)

## ğŸ“ Project Structure

```
RAGdoll/
â”œâ”€â”€ app/                    # FastAPI backend
â”‚   â”œâ”€â”€ api_new.py         # Main API routes
â”‚   â”œâ”€â”€ auth.py            # JWT authentication
â”‚   â”œâ”€â”€ openai_api.py      # GPT-4o integration
â”‚   â”œâ”€â”€ query_namespaced.py # RAG retrieval logic
â”‚   â””â”€â”€ ...
â”œâ”€â”€ TParty/                # React frontend
â”‚   â”œâ”€â”€ src/components/    # React components
â”‚   â”œâ”€â”€ src/services/      # API service layer
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                  # Sample documents
â”œâ”€â”€ docker-compose.fast.yml # Fast development setup
â””â”€â”€ bge-m3_repo/          # Local BGE-M3 model
```

## ğŸ”§ Development

### Fast Build Mode
Uses pre-built layers for rapid iteration:
```bash
docker-compose -f docker-compose.fast.yml up --build
```

### Document Ingestion
Add documents to namespaces via API:
```bash
curl -X POST "http://localhost:8000/ingest" \
  -H "Authorization: Bearer <token>" \
  -F "files=@document.pdf" \
  -F "namespace=engineering"
```

### Namespace Management
- `default`: General documents
- `engineering`: Technical documentation  
- `legal`: Legal and compliance docs
- `hr`: Human resources materials
- `marketing`: Marketing and brand content

## ğŸ” Usage

### Chat Interface (TParty)
1. Login with admin credentials
2. Select a namespace
3. Ask questions about documents in that namespace
4. Get AI-powered responses with source citations

### API Endpoints
- `POST /auth/login` - Authentication
- `POST /query/{namespace}` - Document search
- `POST /api/v1/openai/chat/gpt4` - GPT-4o chat
- `GET /namespaces` - List available namespaces

## ğŸ› ï¸ Configuration

### Environment Variables
```bash
OPENAI_API_KEY=sk-...           # OpenAI API key
OFFLINE_MODE=true               # Use local BGE-M3 model
REDIS_URL=redis://redis:6379/0  # Redis connection
ENABLE_CACHE=true               # Enable response caching
```

### Known Issues
- **SSL Certificate Issues**: Corporate firewalls may block OpenAI API calls
- **Solution**: Configure proxy settings or use offline mode for document retrieval only

## ğŸ“Š Performance
- **Document Retrieval**: Sub-second response times
- **Memory Usage**: ~2GB for BGE-M3 model + application
- **Scalability**: Horizontal scaling via Docker Swarm/Kubernetes

## ğŸ¤ Contributing
1. Use the fast build mode for development
2. Follow the existing code structure
3. Test with sample documents in `data/` directory
4. Update documentation for new features

## ğŸ“„ License
[Add your license here]
